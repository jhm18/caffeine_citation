error_values$paste0("error_", i) <- rnorm(nrow(model_values), mean = 0, sd = sqrt((pi)^2/3))
}
for(i in seq_along(error_values$trial)){
error_values$paste0("error_", i) <- rnorm(nrow(model_values), mean = 0, sd = sqrt((pi)^2/3))
}
i <- 1
error_values$paste0("error_", i) <- rnorm(nrow(model_values), mean = 0, sd = sqrt((pi)^2/3))
error_values <- data.frame(trial=seq(1,num_encode, 1))
for(i in seq_along(error_values$trial)){
error_values$paste0("error_", i) <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
}
error_values <- data.frame(trial=seq(1,num_encode, 1))
for(i in 1:10){
error_values$paste0("error_", i) <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
}
for(i in [1:10]){
for(i in c(1:10)){
error_values$paste0("error_", i) <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
}
for(i in 1:10){
error_values$paste0("error_", i) <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
}
for(i in 1:10){
error_values$paste0("error_", i) <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
}
i in 1:10
for(i in seq(1,10,1)){
error_values$paste0("error_", i) <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
}
error_values$error1 <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
error_values$error2 <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
error_values$error3 <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
error_values$error4 <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
error_values$error5 <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
error_values$error6 <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
error_values$error7 <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
error_values$error8 <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
error_values$error9 <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
error_values$error10 <- rnorm(num_encode, mean = 0, sd = sqrt((pi)^2/3))
View(error_values)
iter_val <- 10
error_values[iter_val, runif(1, 1, 10)]
create_modelvalues <- function(model_values, task_log, num_encode, error_values){
model_values1 <- data.frame(trial=vector("numeric", length=num_encode), bLactivation=vector("numeric", length=num_encode),
recall_prob=vector("numeric", length=num_encode), rt=vector("numeric", length=num_encode))
iter_val <- 1
for(i in 1:nrow(task_log)){
if(task_log[[i,3]] == "encode_chunk"){
current_response <- task_log[i, ]
model_values1$trial[[iter_val]] <- current_response[[1]]
curr_error <- error_values[iter_val, runif(1, 1, 10)]
curr_chunktime <-  task_log[[i,7]] + 12 - unique_chunk_list[[current_response$id,2]]
bl_act <- baseLevelActivation(current_response$presentation,curr_chunktime)
model_values1$bLactivation[[iter_val]] <- bl_act
act <- bl_act + curr_error
model_values1$recall_prob[[iter_val]] <- recall_probability(act)
# Using KLM, .28 seconds per keystroke
model_values$rt[[iter_val]] <- retrieval_time(act) + .28*2
#move to next response
iter_val <- iter_val + 1
} else{}
}
}
rm(data,error,error1,error2,error3,error4,error5,error6,error7,error8,error9,error10)
rm(test,test1)
model_values1 <- data.frame(trial=vector("numeric", length=num_encode), bLactivation=vector("numeric", length=num_encode),
recall_prob=vector("numeric", length=num_encode), rt=vector("numeric", length=num_encode))
create_modelvalues <- function(model_values, task_log, num_encode, error_values){
iter_val <- 1
for(i in 1:nrow(task_log)){
if(task_log[[i,3]] == "encode_chunk"){
current_response <- task_log[i, ]
model_values$trial[[iter_val]] <- current_response[[1]]
curr_error <- error_values[iter_val, runif(1, 1, 10)]
curr_chunktime <-  task_log[[i,7]] + 12 - unique_chunk_list[[current_response$id,2]]
bl_act <- baseLevelActivation(current_response$presentation,curr_chunktime)
model_values$bLactivation[[iter_val]] <- bl_act
act <- bl_act + curr_error
model_values$recall_prob[[iter_val]] <- recall_probability(act)
# Using KLM, .28 seconds per keystroke
model_values$rt[[iter_val]] <- retrieval_time(act) + .28*2
#move to next response
iter_val <- iter_val + 1
} else{}
}
return(model_values)
}
model_values1 <- data.frame(trial=vector("numeric", length=num_encode), bLactivation=vector("numeric", length=num_encode),
recall_prob=vector("numeric", length=num_encode), rt=vector("numeric", length=num_encode))
model_values1 <- create_modelvalues(model_values1, task_log, num_encode, error_values)
View(model_values1)
create_modelvalues <- function(task_log, num_encode, error_values){
model_values <- data.frame(trial=vector("numeric", length=num_encode), bLactivation=vector("numeric", length=num_encode),
recall_prob=vector("numeric", length=num_encode), rt=vector("numeric", length=num_encode))
iter_val <- 1
for(i in 1:nrow(task_log)){
if(task_log[[i,3]] == "encode_chunk"){
current_response <- task_log[i, ]
model_values$trial[[iter_val]] <- current_response[[1]]
curr_error <- error_values[iter_val, runif(1, 1, 10)]
curr_chunktime <-  task_log[[i,7]] + 12 - unique_chunk_list[[current_response$id,2]]
bl_act <- baseLevelActivation(current_response$presentation,curr_chunktime)
model_values$bLactivation[[iter_val]] <- bl_act
act <- bl_act + curr_error
model_values$recall_prob[[iter_val]] <- recall_probability(act)
# Using KLM, .28 seconds per keystroke
model_values$rt[[iter_val]] <- retrieval_time(act) + .28*2
#move to next response
iter_val <- iter_val + 1
} else{}
}
return(model_values)
}
model_values1 <- create_modelvalues(task_log, num_encode, error_values)
View(model_values1)
model_values2 <- create_modelvalues(task_log, num_encode, error_values)
model_values3 <- create_modelvalues(task_log, num_encode, error_values)
model_values4 <- create_modelvalues(task_log, num_encode, error_values)
model_values5 <- create_modelvalues(task_log, num_encode, error_values)
model_values6 <- create_modelvalues(task_log, num_encode, error_values)
View(model_values2)
View(model_values3)
rm(model_values)
View(model_values1)
create_meanrt <- function(model_values){
mean_rt <- vector("numeric", length = 36)
for(i in 1:36){
mean_rt[[i]] <- mean(model_values$rt[model_values$trial == i])
}
model_values$meanrt <- mean_rt
}
create_meanrt <- function(model_values){
mean_rt <- vector("numeric", length = 36)
for(i in 1:36){
mean_rt[[i]] <- mean(model_values$rt[model_values$trial == i])
}
model_values$meanrt <- mean_rt
return(model_values)
}
model_values1 <- create_meanrt(model_values1)
create_meanrt <- function(model_values){
mean_rt <- vector("numeric", length = 36)
for(i in 1:36){
mean_rt[[i]] <- mean(model_values$rt[model_values$trial == i])
}
return(mean_rt)
}
model_values1 <- create_modelvalues(task_log, num_encode, error_values)
mean_rt1 <- create_meanrt(model_values1)
rm(mean_rt)
mean_rt2 <- create_meanrt(model_values2)
mean_rt3 <- create_meanrt(model_values3)
mean_rt4 <- create_meanrt(model_values4)
mean_rt5 <- create_meanrt(model_values5)
mean_rt6 <- create_meanrt(model_values6)
plotting_data <- data.frame(trial=seq(1,36,1), run1=mean_rt1, run2=mean_rt2, run3=mean_rt3,
run4=mean_rt4, run5=mean_rt5, run6=mean_rt6)
ggplot() + geom_point(data = plotting_data, aes(trial, mean_rt1/2)) +
geom_point(data=plotting_data, aes(trial, mean_rt2/2))
plotting_data <- data.frame(trial=seq(1,36,1), run1=mean_rt1/2, run2=mean_rt2/2, run3=mean_rt3/2,
run4=mean_rt4/2, run5=mean_rt5/2, run6=mean_rt6/2)
View(plotting_data)
ggplot() + geom_point(data = plotting_data, aes(trial, mean_rt1)) +
geom_point(data=plotting_data, aes(trial, mean_rt2)) +
geom_point(data=plotting_data, aes(trial, mean_rt3)) +
geom_point(data=plotting_data, aes(trial, mean_rt4)) +
geom_point(data=plotting_data, aes(trial, mean_rt5)) +
geom_point(data=plotting_data, aes(trial, mean_rt6))
plotting_data[c(2,7)] > 7
plotting_data[c(2:7)] > 7 <- "NA"
(plotting_data[c(2:7)] > 7 ) <- "NA"
plotting_data[c(2:7)] > 7
plotting_data[plotting_data[c(2:7)] > 7] = "NA"
plotting_data[plotting_data[c(2:7)] > 7, c(2:7)] = "NA"
is.na(plotting_data$run1) <- df > 7
plotting_data <- data.frame(run1=mean_rt1/2, run2=mean_rt2/2, run3=mean_rt3/2,
run4=mean_rt4/2, run5=mean_rt5/2, run6=mean_rt6/2)
is.na(plotting_data) <- plotting_data > 7
View(plotting_data)
plotting_data$trial <- seq(1,36,1),
plotting_data$trial <- seq(1,36,1)
View(plotting_data)
ggplot() + geom_point(data = plotting_data, aes(trial, mean_rt1)) +
geom_point(data=plotting_data, aes(trial, mean_rt2)) +
geom_point(data=plotting_data, aes(trial, mean_rt3)) +
geom_point(data=plotting_data, aes(trial, mean_rt4)) +
geom_point(data=plotting_data, aes(trial, mean_rt5)) +
geom_point(data=plotting_data, aes(trial, mean_rt6))
geom_point(data=plotting_data, aes(trial, mean_rt3)) +
90
ggplot() + geom_point(data = plotting_data, aes(trial, mean_rt1))
plotting_data <- data.frame(run1=mean_rt1/2, run2=mean_rt2/2, run3=mean_rt3/2,
run4=mean_rt4/2, run5=mean_rt5/2, run6=mean_rt6/2)
plotting_data$trial <- seq(1,36,1)
ggplot() + geom_point(data = plotting_data, aes(trial, run1)) +
geom_point(data=plotting_data, aes(trial, run2)) +
geom_point(data=plotting_data, aes(trial, run3)) +
geom_point(data=plotting_data, aes(trial, run4)) +
geom_point(data=plotting_data, aes(trial, run5)) +
geom_point(data=plotting_data, aes(trial, run6))
plotting_data <- data.frame(run1=mean_rt1/2, run2=mean_rt2/2, run3=mean_rt3/2,
run4=mean_rt4/2, run5=mean_rt5/2, run6=mean_rt6/2)
is.na(plotting_data) <- plotting_data > 7
plotting_data$trial <- seq(1,36,1)
ggplot() + geom_point(data = plotting_data, aes(trial, run1)) +
geom_point(data=plotting_data, aes(trial, run2)) +
geom_point(data=plotting_data, aes(trial, run3)) +
geom_point(data=plotting_data, aes(trial, run4)) +
geom_point(data=plotting_data, aes(trial, run5)) +
geom_point(data=plotting_data, aes(trial, run6))
error_values <- data.frame(trial=seq(1,num_encode, 1))
error_values$error1 <- rnorm(num_encode, mean = 0, sd = (pi)^2/3) * 0.25
error_values$error2 <- rnorm(num_encode, mean = 0, sd = (pi)^2/3) * 0.3
error_values$error3 <- rnorm(num_encode, mean = 0, sd = (pi)^2/3) * 0.28
error_values$error4 <- rnorm(num_encode, mean = 0, sd = (pi)^2/3) * 0.4
error_values$error5 <- rnorm(num_encode, mean = 0, sd = (pi)^2/3) * 0.6
error_values$error6 <- rnorm(num_encode, mean = 0, sd = (pi)^2/3) * 0.2
error_values$error7 <- rnorm(num_encode, mean = 0, sd = (pi)^2/3) * 0.5
error_values$error8 <- rnorm(num_encode, mean = 0, sd = (pi)^2/3) * 0.45
error_values$error9 <- rnorm(num_encode, mean = 0, sd = (pi)^2/3) * 0.55
error_values$error10 <- rnorm(num_encode, mean = 0, sd = (pi)^2/3) * 0.35
num_encode <- nrow(task_log[task_log$state == "encode_chunk", ])
create_modelvalues <- function(task_log, num_encode, error_values){
model_values <- data.frame(trial=vector("numeric", length=num_encode), bLactivation=vector("numeric", length=num_encode),
recall_prob=vector("numeric", length=num_encode), rt=vector("numeric", length=num_encode))
iter_val <- 1
for(i in 1:nrow(task_log)){
if(task_log[[i,3]] == "encode_chunk"){
current_response <- task_log[i, ]
model_values$trial[[iter_val]] <- current_response[[1]]
curr_error <- error_values[iter_val, runif(1, 1, 10)]
curr_chunktime <-  task_log[[i,7]] + 12 - unique_chunk_list[[current_response$id,2]]
bl_act <- baseLevelActivation(current_response$presentation,curr_chunktime)
model_values$bLactivation[[iter_val]] <- bl_act
act <- bl_act + curr_error
model_values$recall_prob[[iter_val]] <- recall_probability(act)
# Using KLM, .28 seconds per keystroke
model_values$rt[[iter_val]] <- retrieval_time(act) + .28*2
#move to next response
iter_val <- iter_val + 1
} else{}
}
return(model_values)
}
model_values1 <- create_modelvalues(task_log, num_encode, error_values)
model_values2 <- create_modelvalues(task_log, num_encode, error_values)
model_values3 <- create_modelvalues(task_log, num_encode, error_values)
model_values4 <- create_modelvalues(task_log, num_encode, error_values)
model_values5 <- create_modelvalues(task_log, num_encode, error_values)
model_values6 <- create_modelvalues(task_log, num_encode, error_values)
create_meanrt <- function(model_values){
mean_rt <- vector("numeric", length = 36)
for(i in 1:36){
mean_rt[[i]] <- mean(model_values$rt[model_values$trial == i])
}
return(mean_rt)
}
mean_rt1 <- create_meanrt(model_values1)
mean_rt2 <- create_meanrt(model_values2)
mean_rt3 <- create_meanrt(model_values3)
mean_rt4 <- create_meanrt(model_values4)
mean_rt5 <- create_meanrt(model_values5)
mean_rt6 <- create_meanrt(model_values6)
plotting_data <- data.frame(run1=mean_rt1/2, run2=mean_rt2/2, run3=mean_rt3/2,
run4=mean_rt4/2, run5=mean_rt5/2, run6=mean_rt6/2)
# remove outliers (over 7sec)
is.na(plotting_data) <- plotting_data > 7
plotting_data$trial <- seq(1,36,1)
ggplot() + geom_point(data = plotting_data, aes(trial, run1)) +
geom_point(data=plotting_data, aes(trial, run2)) +
geom_point(data=plotting_data, aes(trial, run3)) +
geom_point(data=plotting_data, aes(trial, run4)) +
geom_point(data=plotting_data, aes(trial, run5)) +
geom_point(data=plotting_data, aes(trial, run6)) + geom_abline()
ggplot() + geom_point(data = plotting_data, aes(trial, run1)) +
geom_point(data=plotting_data, aes(trial, run2)) +
geom_point(data=plotting_data, aes(trial, run3)) +
geom_point(data=plotting_data, aes(trial, run4)) +
geom_point(data=plotting_data, aes(trial, run5)) +
geom_point(data=plotting_data, aes(trial, run6))
load("~/Downloads/article_combined_March5.Rda")
year_table <- table(article_combined$year)
year_count <- data.frame(year=names(year_table), count=year_table)
year_count <- year_count[-c(2)]
View(year_count)
plot(year_count$year, year_count$count.Freq, bty="l", las=1)
year_count$year <- as.integer(year_count$year)
year_count_high <- year_count[year_count$year >= 1990,]
year_count_low <- year_count[year_count$year < 1990,]
View(year_count_high)
View(year_count_low)
year_count_high <- year_count[year_count$year > 1990,]
year_count_low <- year_count[year_count$year <= 1990,]
plot(year_count_high$year, year_count_high$count.Freq, bty="l", las=1)
plot(year_count_low$year, year_count_low$count.Freq, bty="l", las=1)
View(year_count_low)
era_index <- data.frame(era=seq(1,11,1), start=c(1907,1945,1946,1956,1966,1976,1986,1991,1996,2001,2006),
end=c(1944,1945,1955,1965,1975,1985,1990,1995,2000,2005,2008))
View(era_index)
late_era <- data.frame(era=seq(12,11,1), start=seq(2009,11,1), end=seq(2009,11,1))
seq(12,22,1)
late_era <- data.frame(era=seq(12,22,1), start=seq(2009,2019,1), end=seq(2009,2019,1))
View(late_era)
era_index <- rbind(era_index, late_era)
era <- vector("integer", length= nrow(era_index))
View(year_count)
era <- vector("integer", length= nrow(year_count))
i <- 1
year <- year_count[i,1]
era_index[(year >= era_index$start & year <= era_index$end), 1]
era_index[(year >= era_index$start & year <= era_index$end), ]
for(i in seq_along(era)){
year <- year_count[i,1]
era[[i]] <- era_index[(year >= era_index$start & year <= era_index$end), 1]
}
era_index <- data.frame(era=seq(1,11,1), start=c(1907,1945,1946,1956,1966,1976,1986,1991,1996,2001,2006),
end=c(1944,1945,1955,1965,1975,1985,1990,1995,2000,2005,2008))
late_era <- data.frame(era=seq(12,23,1), start=seq(2009,2020,1), end=seq(2009,2020,1))
era_index <- rbind(era_index, late_era)
era <- vector("integer", length= nrow(year_count))
for(i in seq_along(era)){
year <- year_count[i,1]
era[[i]] <- era_index[(year >= era_index$start & year <= era_index$end), 1]
}
era
time_id <- data.frame(time_id=seq_along(year_count$year), year=year_count$year)
time_id$year <- as.integer(time_id$year)
time_id$era <- era
View(time_id)
View(year_count_high)
time_id <- time_id[time_id$year < 2021, ]
View(time_id)
article_combined <- dplyr::left_join(article_combined, time_id, by=c("year"))
View(article_combined)
load("~/Downloads/article_combined_March5.Rda")
dplyr::left_join(article_combined, time_id, by=c("year"))
test <- dplyr::left_join(article_combined, time_id, by=c("year"))
View(test)
article_combined <-  dplyr::left_join(article_combined, time_id, by=c("year"))
save(article_combined, file="article_combinedv2.Rda")
cat("\014")
# Options
options(stringsAsFactors = FALSE)
options(scipen = 999)
# Set Working Directory
setwd("~/Documents/Caffeine_SNA")
id_map <- function(node_list, vertices, era, network_partition){
# Reduce node list  to current era
curr_node_list <- node_list[node_list$era == era, ]
# Map community to curr_node_list to make id_index using node_id/label
vertices$community <- network_partition
community_nodes <- vertices[c(1,2,6)]
colnames(community_nodes)[c(1,2)] <- c("pajek_id","node_id")
id_index <- dplyr::left_join(curr_node_list, community_nodes, by="node_id")
id_index <- id_index[c(8,1,2,9,7)]
return(id_index)
}
source("RPajekFunctions_30April2023.R")
citation_finder <- function(node_list,edges, era1, era2){
# Returns cited articles found in the current eras
# Join era with edge list
node_index <- node_list[c(1,7)]
colnames(node_index)[[1]] <- c("sender_id")
citation_index <- dplyr::left_join(edges, node_index, by='sender_id')
# Define edge list for each era
citation_index1 <- citation_index[citation_index$era == era1, ]
citation_index2 <- citation_index[citation_index$era == era2, ]
colnames(citation_index1)[[5]] <- c("era_1")
colnames(citation_index2)[[5]] <- c("era_2")
# Make target_id unique in each era
citation_index1 <- citation_index1[!duplicated(citation_index1$target_id), ]
citation_index2 <- citation_index2[!duplicated(citation_index2$target_id), ]
# Identify shared citations
cited_index <- dplyr::left_join(citation_index2, citation_index1[c(3,5)], by='target_id')
cited_index <- cited_index[!is.na(cited_index$era_1),]
return(cited_index)
}
load("~/Documents/Caffeine_SNA/citation_node_list_3Oct2024.Rda")
load("~/Documents/Caffeine_SNA/article_combinedv2.Rda")
load("~/Documents/Caffeine_SNA/citation_edge_list_21Mar2024.Rda")
setwd("~/Documents/Caffeine_SNA/Temporal networks/Era23")
read_clu(getwd(), "era23_testCommunity")
read_net("era23.net")
era23_vertices <- vertices
era23_edges <- ties
era23_communities <- network_partition
#
setwd("~/Documents/Caffeine_SNA/Temporal networks/Era22")
read_clu(getwd(), "era22_testCommunity")
read_net("era22.net")
era22_vertices <- vertices
era22_edges <- ties
era22_communities <- network_partition
rm(network_partition, vertices, ties)
era23_id_index <- id_map(node_list, era23_vertices, 23, era23_communities)
era22_id_index <- id_map(node_list, era22_vertices, 22, era22_communities)
cited_index <- citation_finder(node_list, edges, 22, 23)
shared_index <- cited_index[c(1,3,4)]
colnames(shared_index)[[1]] <- c("pajek_id")
shared_index <- dplyr::left_join(shared_index, t1_id_index[c(1,4,5)], by="pajek_id")
t1_id_index <- era22_id_index
t2_id_index <- era23_id_index
shared_index <- cited_index[c(1,3,4)]
colnames(shared_index)[[1]] <- c("pajek_id")
shared_index <- dplyr::left_join(shared_index, t1_id_index[c(1,4,5)], by="pajek_id")
colnames(shared_index)[c(4,5)] <- c("community1", "era1")
shared_index <- dplyr::left_join(shared_index, t2_id_index[c(1,4,5)], by="pajek_id")
colnames(shared_index)[c(6,7)] <- c("community2", "era2")
shared_index$pair <- apply(shared_index[, c("community1", "community2")], 1, function(x) paste(sort(x), collapse = "-"))
shared_index$count <- 1
result <- aggregate(count ~ pair, data = shared_index, sum)
result_pairs <- strsplit(result$pair, "-")
first_elements <- sapply(result_pairs, `[`, 1)
second_elements <- sapply(result_pairs, `[`, 2)
result <- cbind(result,first_elements,second_elements)
View(result)
unique(result$first_elements)
first_elements_list <- unique(result$first_elements)
proportion <- vector("list", length = length(first_elements_list))
i <- 1
curr_element <- result[result$first_elements == first_elements_list[[i]], ]
View(curr_element)
curr_sum <- sum(curr_element$count)
curr_element$proportion <- curr_element$count/curr_sum
View(curr_element)
curr_element <- curr_element[order(curr_element$proportion),]
View(curr_element)
curr_element <- curr_element[order(curr_element$proportion, decreasing = TRUE),]
View(curr_element)
for(i in seq_along(first_elements_list)){
curr_element <- result[result$first_elements == first_elements_list[[i]], ]
curr_sum <- sum(curr_element$count)
curr_element$proportion <- curr_element$count/curr_sum
curr_element <- curr_element[order(curr_element$proportion, decreasing = TRUE),]
proportion[[i]] <- curr_element
}
View(proportion)
result2 <- do.call("rbind", proportion)
View(result2)
result$first_elements <- as.integer(result$first_elements)
result$second_elements <- as.integer(result$second_elements)
first_elements_list <- unique(result$first_elements)
proportion <- vector("list", length = length(first_elements_list))
for(i in seq_along(first_elements_list)){
curr_element <- result[result$first_elements == first_elements_list[[i]], ]
curr_sum <- sum(curr_element$count)
curr_element$proportion <- curr_element$count/curr_sum
curr_element <- curr_element[order(curr_element$proportion, decreasing = TRUE),]
proportion[[i]] <- curr_element
}
result2 <- do.call("rbind", proportion)
View(result2)
View(result2)
result <- result[order(result$first_elements,result$second_elements), ]
View(result)
first_elements_list <- unique(result$first_elements)
proportion <- vector("list", length = length(first_elements_list))
for(i in seq_along(first_elements_list)){
curr_element <- result[result$first_elements == first_elements_list[[i]], ]
curr_sum <- sum(curr_element$count)
curr_element$proportion <- curr_element$count/curr_sum
curr_element <- curr_element[order(curr_element$proportion, decreasing = TRUE),]
proportion[[i]] <- curr_element
}
result2 <- do.call("rbind", proportion)
View(result2)
community_membership <- function(t1_id_index, t2_id_index, cited_index){
# Make a shared index for citations that span eras
shared_index <- cited_index[c(1,3,4)]
colnames(shared_index)[[1]] <- c("pajek_id")
# Make unique senders list
#shared_senders <- data.frame(pajek_id =unique(shared_index$pajek_id))
#shared_senders <- dplyr::left_join(shared_senders, t1_id_index[c(1,4,5)], by="pajek_id")
# Add community to cited index for each era
shared_index <- dplyr::left_join(shared_index, t1_id_index[c(1,4,5)], by="pajek_id")
colnames(shared_index)[c(4,5)] <- c("community1", "era1")
shared_index <- dplyr::left_join(shared_index, t2_id_index[c(1,4,5)], by="pajek_id")
colnames(shared_index)[c(6,7)] <- c("community2", "era2")
# Aggregate unique community pairs from cols community1 and community2
shared_index$pair <- apply(shared_index[, c("community1", "community2")], 1, function(x) paste(sort(x), collapse = "-"))
# Aggregate the sum by unique pairs
shared_index$count <- 1
# Aggregate the row counts for each unique pair
result <- aggregate(count ~ pair, data = shared_index, sum)
result_pairs <- strsplit(result$pair, "-")
# First element of each pair
first_elements <- sapply(result_pairs, `[`, 1)
# Second element of each pair
second_elements <- sapply(result_pairs, `[`, 2)
result <- cbind(result,first_elements,second_elements)
result$first_elements <- as.integer(result$first_elements)
result$second_elements <- as.integer(result$second_elements)
result <- result[order(result$first_elements,result$second_elements), ]
# Calculate proportion for each first element
first_elements_list <- unique(result$first_elements)
proportion <- vector("list", length = length(first_elements_list))
for(i in seq_along(first_elements_list)){
curr_element <- result[result$first_elements == first_elements_list[[i]], ]
curr_sum <- sum(curr_element$count)
curr_element$proportion <- curr_element$count/curr_sum
curr_element <- curr_element[order(curr_element$proportion, decreasing = TRUE),]
proportion[[i]] <- curr_element
}
result2 <- do.call("rbind", proportion)
return(result2)
}
eras_edges <- community_membership(era22_id_index, era23_id_index, cited_index)
View(eras_edges)
eras_first_elements <- paste0("era1_",eras_edges$first_elements)
eras_second_elements <- paste0("era2_", eras_edges$second_elements)
eras_nodes <- unique(c(eras_first_elements, eras_second_elements))
eras_nodes
